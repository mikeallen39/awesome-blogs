# Awesome Blogs 📚

我将在这个 repo 中记录自己看过的一些优质文章，包括但不限于 blog、技术文档、微信公众号文章、知乎文章等。

---

## Something Wonderful ✨

- [developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)：开发者学习路线图，非常全面！！
- [【精校版】Andrej Karpathy微软Build大会精彩演讲](https://www.bilibili.com/video/BV1ts4y1T7UH/)：大神Andrej Karpthy在微软Build 2023开发者大会上做的关于GPT的专题演讲视频。

---

## Comprehensive Study 📖

- [A Meticulous Guide to Advances in Deep Learning Efficiency over the Years](https://alexzhang13.github.io/blog/2024/efficient-dl/)：这篇文章系统性地回顾了深度学习的发展历史，包括算法、硬件（GPU等）、库、编译器等内容，是一篇入门深度学习的不错选择。
- [The 2025 AI Engineer Reading List](https://www.latent.space/p/2025-papers)：Latent.Space总结了选择了 50 篇/模型/博客，涵盖 AI 领域的 10 个方向：LLMs、基准、提示、RAG、代理、CodeGen、视觉、语音、扩散、微调。
- [Things we learned about LLMs in 2024](https://simonwillison.net/2024/Dec/31/llms-in-2024/?continueFlag=f625166ef792bb9d445c5e81716c454e)：2024 年大型语言模型领域回顾。
- [大模型MLSYS学习随笔- 训推框架总览](https://zhuanlan.zhihu.com/p/692438094)：

---

## DL Basic 🧠

- [PyTorch internals](http://blog.ezyang.com/2019/05/pytorch-internals/?continueFlag=f625166ef792bb9d445c5e81716c454e)：一篇长文深入解析 PyTorch 内部机制，文章内容包括张量数据结构、自动微分原理、PyTorch 代码结构和编写内核的实用工具等，旨在帮助读者更好地理解 PyTorch 的工作原理。
  
- [Transformer Math 101](https://blog.eleuther.ai/transformer-math/?continueFlag=f625166ef792bb9d445c5e81716c454e)：深入讲解了 Transformer 中的各种数学计算，详细解释了训练 Transformer 模型所需的计算量和内存需求，并讨论了参数与数据集大小的权衡、工程计算成本、内存需求、推理、训练、分布式训练等方面的具体计算方法。

---

## MOE 🤖

- [A Visual Guide to Mixture of Experts (MoE)](newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)：一篇非常全面地讲解 MOE 的文章，文中包含了大量的可视化，简单易懂！
- [详细谈谈DeepSeek MoE相关的技术发展](https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w)：一篇长文，详细讨论了DeepSeek MoE技术的发展历程及其优化策略。

---

## 推理加速 ⚡

- [图解大模型计算加速系列：FlashAttention V1](https://www.zhihu.com/question/591646269/answer/3309904882?utm_psn=1858655279132069888)：非常细致地讲解了 FlashAttention。

---

## 扩散模型 🌌

- [Diffusion Models - bit by bit](https://lunar-joke-35b.notion.site/Diffusion-Models-bit-by-bit-10fba4b6a3fa80458d16e58036875747)：入门级讲解 Diffusion Model。



## 分布式训练 🚀

- [关于大模型分布式训练的一些个人总结](https://zhuanlan.zhihu.com/p/699021260?utm_psn=1858827910254829569)：对分布式训练进行了详细的总结。
- [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook)：Huggingface分布式系统教程，涵盖DP、ZeRO、TP等等知识。
- [终极训练指南： 在大规模 GPU 集群上训练大语言模型](https://huggingface.co/spaces/Ki-Seki/ultrascale-playbook-zh-cn)：HuggingFace分布式训练教程的中文翻译版。



## 经验/感受 👣

- [编程十年的感悟](https://ramsayleung.github.io/zh/post/2024/%E7%BC%96%E7%A8%8B%E5%8D%81%E5%B9%B4%E7%9A%84%E6%84%9F%E6%82%9F/?continueFlag=f625166ef792bb9d445c5e81716c454e)：Ramsay 分享自己的一些人生经验，关于编程等等。
- [ 深度学习调优指南中文版](https://sourcecode.gitbook.io/ai?continueFlag=db7218f532c52505e06318f0e1982660)：系统地教你将深度学习模型的性能最大化，重点是超参数调优的过程。